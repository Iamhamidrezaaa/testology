#!/usr/bin/env python3
"""
ğŸŒ Ø³ÛŒØ³ØªÙ… Collective Intelligence - Testology
Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ Ú©Ù‡ Ø§Ø² Ø±ÙØªØ§Ø± Ùˆ Ø§Ø­Ø³Ø§Ø³Ø§Øª ØªÙ…Ø§Ù… Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ø¯
"""

import json
import os
import sys
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from collections import Counter, defaultdict
import random

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Import dependencies
try:
    from neural_memory import add_memory, retrieve_memory, get_memory_stats
    MEMORY_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± import: {e}")
    MEMORY_AVAILABLE = False

# Import ML libraries
try:
    from sklearn.cluster import KMeans, DBSCAN
    from sklearn.preprocessing import StandardScaler
    from sklearn.decomposition import PCA
    from sklearn.metrics import silhouette_score
    ML_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± import ML libraries: {e}")
    ML_AVAILABLE = False

DATA_PATH = "ml/data/collective_data.json"
REPORT_PATH = "ml/data/collective_report.json"
TRENDS_PATH = "ml/data/social_trends.json"

def generate_sample_data():
    """ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªØ³Øª Ø³ÛŒØ³ØªÙ…"""
    print("ğŸŒ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø±Ø§ÛŒ Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ...")
    
    # Ù…Ù†Ø§Ø·Ù‚ Ù…Ø®ØªÙ„Ù
    regions = ["ØªÙ‡Ø±Ø§Ù†", "Ø§ØµÙÙ‡Ø§Ù†", "Ø´ÛŒØ±Ø§Ø²", "Ù…Ø´Ù‡Ø¯", "ØªØ¨Ø±ÛŒØ²", "Ú©Ø±Ø¬", "Ø§Ù‡ÙˆØ§Ø²", "Ù‚Ù…"]
    
    # Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
    languages = ["fa", "en", "ar", "tr"]
    
    # ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
    sample_data = []
    base_date = datetime.now() - timedelta(days=30)
    
    for i in range(1000):  # 1000 Ú©Ø§Ø±Ø¨Ø± Ù†Ù…ÙˆÙ†Ù‡
        # ØªØ§Ø±ÛŒØ® ØªØµØ§Ø¯ÙÛŒ Ø¯Ø± 30 Ø±ÙˆØ² Ú¯Ø°Ø´ØªÙ‡
        random_days = random.randint(0, 30)
        timestamp = base_date + timedelta(days=random_days)
        
        # ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ§Ù†ÛŒ
        user_data = {
            "userId": f"user_{i+1:04d}",
            "timestamp": timestamp.isoformat(),
            "region": random.choice(regions),
            "language": random.choice(languages),
            "age": random.randint(18, 65),
            "gender": random.choice(["male", "female", "other"]),
            
            # ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ§Ù†ÛŒ
            "anxiety": random.uniform(0.1, 1.0),
            "depression": random.uniform(0.1, 1.0),
            "stress": random.uniform(0.1, 1.0),
            "happiness": random.uniform(0.1, 1.0),
            "satisfaction": random.uniform(0.1, 1.0),
            "confidence": random.uniform(0.1, 1.0),
            "social_support": random.uniform(0.1, 1.0),
            "life_quality": random.uniform(0.1, 1.0),
            
            # Ø±ÙØªØ§Ø± Ø¢Ù†Ù„Ø§ÛŒÙ†
            "session_duration": random.randint(5, 120),  # Ø¯Ù‚ÛŒÙ‚Ù‡
            "tests_completed": random.randint(1, 10),
            "articles_read": random.randint(0, 20),
            "chat_interactions": random.randint(0, 15),
            
            # Ø§Ø­Ø³Ø§Ø³Ø§Øª
            "mood_score": random.uniform(0.1, 1.0),
            "energy_level": random.uniform(0.1, 1.0),
            "motivation": random.uniform(0.1, 1.0),
            
            # Ø¹ÙˆØ§Ù…Ù„ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ
            "social_anxiety": random.uniform(0.1, 1.0),
            "loneliness": random.uniform(0.1, 1.0),
            "belonging": random.uniform(0.1, 1.0)
        }
        
        sample_data.append(user_data)
    
    # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    os.makedirs("ml/data", exist_ok=True)
    with open(DATA_PATH, 'w', encoding='utf-8') as f:
        json.dump(sample_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… {len(sample_data)} Ø±Ú©ÙˆØ±Ø¯ Ù†Ù…ÙˆÙ†Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯")
    return sample_data

def load_collective_data():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹ÛŒ"""
    try:
        if not os.path.exists(DATA_PATH):
            print("ğŸ“Š Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª. ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡...")
            return generate_sample_data()
        
        with open(DATA_PATH, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        print(f"ğŸ“Š {len(data)} Ø±Ú©ÙˆØ±Ø¯ Ø¬Ù…Ø¹ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯")
        return data
        
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹ÛŒ: {e}")
        return []

def analyze_collective_psychology(data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ÛŒ Ø¬Ù…Ø¹ÛŒ"""
    if not data:
        return {"status": "empty", "message": "Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯"}
    
    df = pd.DataFrame(data)
    
    # ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±ÛŒ Ú©Ù„ÛŒ
    psychological_metrics = [
        'anxiety', 'depression', 'stress', 'happiness', 'satisfaction', 
        'confidence', 'social_support', 'life_quality', 'mood_score',
        'energy_level', 'motivation', 'social_anxiety', 'loneliness', 'belonging'
    ]
    
    stats = {}
    for metric in psychological_metrics:
        if metric in df.columns:
            stats[metric] = {
                'mean': float(df[metric].mean()),
                'std': float(df[metric].std()),
                'min': float(df[metric].min()),
                'max': float(df[metric].max()),
                'median': float(df[metric].median())
            }
    
    # ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ø³Ù†ÛŒ
    age_distribution = df['age'].value_counts().to_dict()
    
    # ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ø¬Ù†Ø³ÛŒØªÛŒ
    gender_distribution = df['gender'].value_counts().to_dict()
    
    # ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ
    region_distribution = df['region'].value_counts().to_dict()
    
    # ØªØ­Ù„ÛŒÙ„ ØªÙˆØ²ÛŒØ¹ Ø²Ø¨Ø§Ù†ÛŒ
    language_distribution = df['language'].value_counts().to_dict()
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµ Ø³Ù„Ø§Ù…Øª Ø±ÙˆØ§Ù† Ø¬Ù…Ø¹ÛŒ
    mental_health_index = (
        stats.get('happiness', {}).get('mean', 0.5) +
        stats.get('satisfaction', {}).get('mean', 0.5) +
        stats.get('life_quality', {}).get('mean', 0.5) +
        (1 - stats.get('anxiety', {}).get('mean', 0.5)) +
        (1 - stats.get('depression', {}).get('mean', 0.5))
    ) / 5
    
    return {
        "psychological_stats": stats,
        "age_distribution": age_distribution,
        "gender_distribution": gender_distribution,
        "region_distribution": region_distribution,
        "language_distribution": language_distribution,
        "mental_health_index": mental_health_index,
        "total_users": len(df),
        "analysis_timestamp": datetime.now().isoformat()
    }

def perform_clustering_analysis(data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Ø§Ù†Ø¬Ø§Ù… ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ"""
    if not data or not ML_AVAILABLE:
        return {"status": "no_ml", "message": "Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ML Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"}
    
    df = pd.DataFrame(data)
    
    # Ø§Ù†ØªØ®Ø§Ø¨ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
    numeric_features = [
        'anxiety', 'depression', 'stress', 'happiness', 'satisfaction',
        'confidence', 'social_support', 'life_quality', 'mood_score',
        'energy_level', 'motivation', 'social_anxiety', 'loneliness', 'belonging'
    ]
    
    # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
    available_features = [col for col in numeric_features if col in df.columns]
    
    if len(available_features) < 3:
        return {"status": "insufficient_features", "message": "ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯"}
    
    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    X = df[available_features].fillna(0)
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    
    # ØªØ¹ÛŒÛŒÙ† ØªØ¹Ø¯Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
    best_k = 3
    best_score = -1
    
    for k in range(2, min(8, len(df) // 10 + 1)):
        if k >= len(df):
            break
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X_scaled)
        if len(set(cluster_labels)) > 1:  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø¨ÛŒØ´ Ø§Ø² ÛŒÚ© Ø®ÙˆØ´Ù‡
            score = silhouette_score(X_scaled, cluster_labels)
            if score > best_score:
                best_score = score
                best_k = k
    
    # Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
    kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
    df['cluster'] = kmeans.fit_predict(X_scaled)
    
    # ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
    cluster_analysis = {}
    for cluster_id in range(best_k):
        cluster_data = df[df['cluster'] == cluster_id]
        
        cluster_analysis[cluster_id] = {
            'size': len(cluster_data),
            'percentage': len(cluster_data) / len(df) * 100,
            'characteristics': {}
        }
        
        # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡Ø± Ø®ÙˆØ´Ù‡
        for feature in available_features:
            cluster_analysis[cluster_id]['characteristics'][feature] = {
                'mean': float(cluster_data[feature].mean()),
                'std': float(cluster_data[feature].std())
            }
    
    # ØªØ¹Ø±ÛŒÙ Ø´Ø®ØµÛŒØª Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§
    cluster_personalities = {}
    for cluster_id, analysis in cluster_analysis.items():
        char = analysis['characteristics']
        
        # ØªØ¹ÛŒÛŒÙ† Ø´Ø®ØµÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
        if (char.get('happiness', {}).get('mean', 0.5) > 0.7 and 
            char.get('satisfaction', {}).get('mean', 0.5) > 0.7):
            personality = "Ù…Ø«Ø¨Øªâ€ŒÚ¯Ø±Ø§ Ùˆ Ø±Ø§Ø¶ÛŒ"
        elif (char.get('anxiety', {}).get('mean', 0.5) > 0.7 or 
              char.get('depression', {}).get('mean', 0.5) > 0.7):
            personality = "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø­Ù…Ø§ÛŒØª"
        elif (char.get('social_support', {}).get('mean', 0.5) > 0.7 and 
              char.get('belonging', {}).get('mean', 0.5) > 0.7):
            personality = "Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ùˆ Ù…ØªØµÙ„"
        else:
            personality = "Ù…ØªØ¹Ø§Ø¯Ù„"
        
        cluster_personalities[cluster_id] = personality
        cluster_analysis[cluster_id]['personality'] = personality
    
    return {
        "clusters": cluster_analysis,
        "cluster_personalities": cluster_personalities,
        "silhouette_score": best_score,
        "optimal_clusters": best_k,
        "features_used": available_features
    }

def analyze_social_trends(data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ"""
    if not data:
        return {"status": "empty"}
    
    df = pd.DataFrame(data)
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    
    # ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
    df['date'] = df['timestamp'].dt.date.astype(str)
    daily_stats = df.groupby('date').agg({
        'anxiety': 'mean',
        'depression': 'mean',
        'happiness': 'mean',
        'satisfaction': 'mean',
        'userId': 'count'
    }).rename(columns={'userId': 'daily_users'})
    
    # ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ù…Ù†Ø·Ù‚Ù‡â€ŒØ§ÛŒ
    regional_stats = df.groupby('region').agg({
        'anxiety': 'mean',
        'depression': 'mean',
        'happiness': 'mean',
        'satisfaction': 'mean',
        'userId': 'count'
    }).rename(columns={'userId': 'users_count'})
    
    # ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø³Ù†ÛŒ
    df['age_group'] = pd.cut(df['age'], bins=[0, 25, 35, 50, 100], labels=['Ø¬ÙˆØ§Ù†', 'Ù…ÛŒØ§Ù†Ø³Ø§Ù„ Ø¬ÙˆØ§Ù†', 'Ù…ÛŒØ§Ù†Ø³Ø§Ù„', 'Ù…Ø³Ù†'])
    age_stats = df.groupby('age_group').agg({
        'anxiety': 'mean',
        'depression': 'mean',
        'happiness': 'mean',
        'satisfaction': 'mean',
        'userId': 'count'
    }).rename(columns={'userId': 'users_count'})
    
    # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ù†Ú¯Ø±Ø§Ù†â€ŒÚ©Ù†Ù†Ø¯Ù‡
    concerning_trends = []
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø·Ø­ Ø§Ø¶Ø·Ø±Ø§Ø¨ Ø¨Ø§Ù„Ø§
    high_anxiety_regions = regional_stats[regional_stats['anxiety'] > 0.7].index.tolist()
    if high_anxiety_regions:
        concerning_trends.append(f"Ø³Ø·Ø­ Ø§Ø¶Ø·Ø±Ø§Ø¨ Ø¨Ø§Ù„Ø§ Ø¯Ø± Ù…Ù†Ø§Ø·Ù‚: {', '.join(high_anxiety_regions)}")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø·Ø­ Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ Ø¨Ø§Ù„Ø§
    high_depression_regions = regional_stats[regional_stats['depression'] > 0.7].index.tolist()
    if high_depression_regions:
        concerning_trends.append(f"Ø³Ø·Ø­ Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ Ø¨Ø§Ù„Ø§ Ø¯Ø± Ù…Ù†Ø§Ø·Ù‚: {', '.join(high_depression_regions)}")
    
    # Ø¨Ø±Ø±Ø³ÛŒ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ
    recent_anxiety = daily_stats['anxiety'].tail(7).mean()
    if recent_anxiety > 0.6:
        concerning_trends.append("Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø¶Ø·Ø±Ø§Ø¨ Ø¯Ø± Ù‡ÙØªÙ‡ Ø§Ø®ÛŒØ±")
    
    return {
        "daily_trends": daily_stats.to_dict('index'),
        "regional_trends": regional_stats.to_dict('index'),
        "age_trends": age_stats.to_dict('index'),
        "concerning_trends": concerning_trends,
        "trend_analysis_timestamp": datetime.now().isoformat()
    }

def generate_collective_insights(psychology_analysis: Dict, clustering_analysis: Dict, trends_analysis: Dict) -> List[str]:
    """ØªÙˆÙ„ÛŒØ¯ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹ÛŒ"""
    insights = []
    
    # Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ÛŒ
    mental_health_index = psychology_analysis.get('mental_health_index', 0.5)
    if mental_health_index > 0.7:
        insights.append("ğŸŒ Ø³Ù„Ø§Ù…Øª Ø±ÙˆØ§Ù† Ø¬Ù…Ø¹ÛŒ Ø¯Ø± Ø³Ø·Ø­ Ù…Ø·Ù„ÙˆØ¨ÛŒ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯")
    elif mental_health_index < 0.4:
        insights.append("âš ï¸ Ø³Ù„Ø§Ù…Øª Ø±ÙˆØ§Ù† Ø¬Ù…Ø¹ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙˆØ¬Ù‡ ÙÙˆØ±ÛŒ Ø¯Ø§Ø±Ø¯")
    else:
        insights.append("ğŸ“Š Ø³Ù„Ø§Ù…Øª Ø±ÙˆØ§Ù† Ø¬Ù…Ø¹ÛŒ Ø¯Ø± Ø³Ø·Ø­ Ù…ØªÙˆØ³Ø· Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ø¯")
    
    # Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
    if clustering_analysis.get('clusters'):
        total_users = sum(cluster['size'] for cluster in clustering_analysis['clusters'].values())
        for cluster_id, cluster_data in clustering_analysis['clusters'].items():
            percentage = cluster_data['percentage']
            personality = cluster_data.get('personality', 'Ù†Ø§Ù…Ø´Ø®Øµ')
            insights.append(f"ğŸ‘¥ {percentage:.1f}% Ø§Ø² Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¯Ø± Ú¯Ø±ÙˆÙ‡ '{personality}' Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯")
    
    # Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ ØªØ±Ù†Ø¯Ù‡Ø§
    concerning_trends = trends_analysis.get('concerning_trends', [])
    if concerning_trends:
        insights.append("ğŸš¨ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ù†Ú¯Ø±Ø§Ù†â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯:")
        for trend in concerning_trends:
            insights.append(f"   â€¢ {trend}")
    else:
        insights.append("âœ… ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ Ø¯Ø± ÙˆØ¶Ø¹ÛŒØª Ù…Ø·Ù„ÙˆØ¨ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯")
    
    # Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯
    insights.append("ğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª Ø¨Ù‡Ø¨ÙˆØ¯:")
    if mental_health_index < 0.5:
        insights.append("   â€¢ Ø§ÙØ²Ø§ÛŒØ´ Ù…Ø­ØªÙˆØ§ÛŒ Ú©Ø§Ù‡Ø´ Ø§Ø¶Ø·Ø±Ø§Ø¨")
        insights.append("   â€¢ Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Ú©Ù…Ù¾ÛŒÙ†â€ŒÙ‡Ø§ÛŒ Ø³Ù„Ø§Ù…Øª Ø±ÙˆØ§Ù†")
    
    if concerning_trends:
        insights.append("   â€¢ ØªÙ…Ø±Ú©Ø² Ø¨Ø± Ù…Ù†Ø§Ø·Ù‚ Ø¨Ø§ Ø³Ø·Ø­ Ø§Ø¶Ø·Ø±Ø§Ø¨ Ø¨Ø§Ù„Ø§")
        insights.append("   â€¢ Ø·Ø±Ø§Ø­ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ‡Ø§ÛŒ Ø­Ù…Ø§ÛŒØªÛŒ ÙˆÛŒÚ˜Ù‡")
    
    return insights

def analyze_collective_intelligence() -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ú©Ø§Ù…Ù„ Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ"""
    print("ğŸŒ Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ Testology...")
    print("ğŸ§  ØªØ¨Ø¯ÛŒÙ„ Ø§Ø² 'I think' Ø¨Ù‡ 'We think'...")
    print("=" * 60)
    
    try:
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        data = load_collective_data()
        if not data:
            return {"status": "error", "message": "Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯"}
        
        print(f"ğŸ“Š ØªØ­Ù„ÛŒÙ„ {len(data)} Ø±Ú©ÙˆØ±Ø¯ Ø¬Ù…Ø¹ÛŒ...")
        
        # ØªØ­Ù„ÛŒÙ„ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ÛŒ Ø¬Ù…Ø¹ÛŒ
        print("ğŸ§  ØªØ­Ù„ÛŒÙ„ Ø±ÙˆØ§Ù†Ø´Ù†Ø§Ø³ÛŒ Ø¬Ù…Ø¹ÛŒ...")
        psychology_analysis = analyze_collective_psychology(data)
        
        # ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ
        print("ğŸ‘¥ ØªØ­Ù„ÛŒÙ„ Ø®ÙˆØ´Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø§Ù†...")
        clustering_analysis = perform_clustering_analysis(data)
        
        # ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ
        print("ğŸ“ˆ ØªØ­Ù„ÛŒÙ„ ØªØ±Ù†Ø¯Ù‡Ø§ÛŒ Ø§Ø¬ØªÙ…Ø§Ø¹ÛŒ...")
        trends_analysis = analyze_social_trends(data)
        
        # ØªÙˆÙ„ÛŒØ¯ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹ÛŒ
        print("ğŸ’¡ ØªÙˆÙ„ÛŒØ¯ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ø¬Ù…Ø¹ÛŒ...")
        insights = generate_collective_insights(psychology_analysis, clustering_analysis, trends_analysis)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
        collective_report = {
            "timestamp": datetime.now().isoformat(),
            "total_users": len(data),
            "psychology_analysis": psychology_analysis,
            "clustering_analysis": clustering_analysis,
            "trends_analysis": trends_analysis,
            "collective_insights": insights,
            "collective_intelligence_level": "high" if len(insights) > 5 else "medium",
            "status": "success"
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        os.makedirs("ml/data", exist_ok=True)
        with open(REPORT_PATH, 'w', encoding='utf-8') as f:
            json.dump(collective_report, f, ensure_ascii=False, indent=2)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
        if MEMORY_AVAILABLE:
            try:
                add_memory(
                    "collective_intelligence",
                    f"ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯. {len(data)} Ú©Ø§Ø±Ø¨Ø±ØŒ {len(insights)} Ø¨ÛŒÙ†Ø´",
                    {
                        "total_users": len(data),
                        "insights_count": len(insights),
                        "mental_health_index": psychology_analysis.get('mental_health_index', 0),
                        "clusters_count": len(clustering_analysis.get('clusters', {}))
                    }
                )
            except Exception as e:
                print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§ÙØ¸Ù‡: {e}")
        
        print("âœ… ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
        print(f"ğŸŒ Testology Ø­Ø§Ù„Ø§ ÛŒÚ© Ù…ØºØ² Ø¬Ù…Ø¹ÛŒ Ø§Ø³Øª!")
        print(f"ğŸ“Š {len(insights)} Ø¨ÛŒÙ†Ø´ Ø¬Ù…Ø¹ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯")
        
        return collective_report
        
    except Exception as e:
        error_report = {
            "status": "error",
            "message": f"Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ: {e}")
        return error_report

def get_collective_stats() -> Dict[str, Any]:
    """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ"""
    try:
        if not os.path.exists(REPORT_PATH):
            return {"status": "no_report", "message": "Ú¯Ø²Ø§Ø±Ø´ Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†ÛŒØ³Øª"}
        
        with open(REPORT_PATH, 'r', encoding='utf-8') as f:
            report = json.load(f)
        
        return {
            "status": "success",
            "stats": {
                "total_users": report.get('total_users', 0),
                "insights_count": len(report.get('collective_insights', [])),
                "mental_health_index": report.get('psychology_analysis', {}).get('mental_health_index', 0),
                "clusters_count": len(report.get('clustering_analysis', {}).get('clusters', {})),
                "concerning_trends": len(report.get('trends_analysis', {}).get('concerning_trends', [])),
                "last_analysis": report.get('timestamp', 'Ù†Ø§Ù…Ø´Ø®Øµ')
            }
        }
        
    except Exception as e:
        return {"status": "error", "message": f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø±: {e}"}

if __name__ == "__main__":
    print("ğŸŒ Ø³ÛŒØ³ØªÙ… Collective Intelligence - Testology")
    print("ğŸ§  ØªØ¨Ø¯ÛŒÙ„ Ø§Ø² Ù‡ÙˆØ´ ÙØ±Ø¯ÛŒ Ø¨Ù‡ Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ")
    print("=" * 50)
    
    # Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ
    report = analyze_collective_intelligence()
    
    # Ù†Ù…Ø§ÛŒØ´ Ù†ØªØ§ÛŒØ¬
    if report.get('status') == 'success':
        print(f"\nğŸ“Š Ø¢Ù…Ø§Ø± Ù‡ÙˆØ´ Ø¬Ù…Ø¹ÛŒ:")
        print(f"   Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ú©Ù„: {report.get('total_users', 0)}")
        print(f"   Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§: {len(report.get('collective_insights', []))}")
        print(f"   Ø´Ø§Ø®Øµ Ø³Ù„Ø§Ù…Øª Ø±ÙˆØ§Ù†: {report.get('psychology_analysis', {}).get('mental_health_index', 0):.3f}")
        print(f"   Ø®ÙˆØ´Ù‡â€ŒÙ‡Ø§: {len(report.get('clustering_analysis', {}).get('clusters', {}))}")
        
        print(f"\nğŸŒ Testology Ø­Ø§Ù„Ø§ ÛŒÚ© Ù…ØºØ² Ø¬Ù…Ø¹ÛŒ Ø§Ø³Øª!")
    else:
        print(f"âŒ Ø®Ø·Ø§: {report.get('message', 'Ø®Ø·Ø§ÛŒ Ù†Ø§Ù…Ø´Ø®Øµ')}")
