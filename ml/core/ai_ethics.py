#!/usr/bin/env python3
"""
ğŸ•Šï¸ Ø³ÛŒØ³ØªÙ… AI Ethics & Self-Regulation - Testology
Ú†Ø§Ø±Ú†ÙˆØ¨ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ùˆ Ø®ÙˆØ¯ØªÙ†Ø¸ÛŒÙ… Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ
"""

import json
import os
import sys
import re
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from collections import defaultdict

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

# Import dependencies
try:
    from neural_memory import add_memory, retrieve_memory, get_memory_stats
    MEMORY_AVAILABLE = True
except ImportError as e:
    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± import: {e}")
    MEMORY_AVAILABLE = False

# Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ù…Ø¤Ø«Ø±
RULES = [
    {"id": 1, "pattern": r"(Ø®ÙˆØ¯Ú©Ø´ÛŒ|Ù…Ø±Ú¯|Ø¢Ø³ÛŒØ¨|Ø®ÙˆØ¯Ø¢Ø²Ø§Ø±ÛŒ)", "action": "alert", "level": "critical", "message": "âš ï¸ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø­Ø³Ø§Ø³ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø¨Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ ÙˆØ§Ù‚Ø¹ÛŒ ØµØ­Ø¨Øª Ú©Ù†ÛŒØ¯."},
    {"id": 2, "pattern": r"(Ø¯Ø§Ø±Ùˆ|ØªØ¬ÙˆÛŒØ²|Ø¯Ø±Ù…Ø§Ù† Ù¾Ø²Ø´Ú©ÛŒ|Ù†Ø³Ø®Ù‡)", "action": "deny", "level": "warning", "message": "ğŸš« Ù…Ù† Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…Ø³Ø§Ø¦Ù„ Ø¯Ø§Ø±ÙˆÛŒÛŒ ÛŒØ§ Ù¾Ø²Ø´Ú©ÛŒ Ø§Ø¸Ù‡Ø§Ø± Ù†Ø¸Ø± Ú©Ù†Ù…."},
    {"id": 3, "pattern": r"(Ù…Ø°Ù‡Ø¨|Ø³ÛŒØ§Ø³Øª|Ø¬Ù†Ø³ÛŒØª|Ø­Ø²Ø¨)", "action": "redirect", "level": "neutral", "message": "ğŸ’¬ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ùˆ ØªØ¬Ø±Ø¨Ù‡â€ŒÙ‡Ø§ÛŒØª ØµØ­Ø¨Øª Ú©Ù†ÛŒÙ…ØŒ Ù†Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø®Ø§Ø±Ø¬ Ø§Ø² Ø±ÙˆØ§Ù†â€ŒØ´Ù†Ø§Ø³ÛŒ."},
    {"id": 4, "pattern": r"(Ø¢Ø¯Ø±Ø³|ØªÙ„ÙÙ†|Ú©Ø¯ Ù…Ù„ÛŒ|Ø´Ù…Ø§Ø±Ù‡ Ú©Ø§Ø±Øª)", "action": "deny", "level": "warning", "message": "ğŸ”’ Ù…Ù† Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø®ØµÛŒ Ø´Ù…Ø§ Ø±Ø§ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ù†Ù…."},
    {"id": 5, "pattern": r".*", "action": "approve", "level": "safe", "message": ""}
]

# Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
ETHICS_PATTERNS = {
    "suicidal_ideation": [
        r"Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù….*Ø¨Ù…ÛŒØ±Ù…",
        r"Ø²Ù†Ø¯Ú¯ÛŒ.*Ø¨ÛŒâ€ŒÙ…Ø¹Ù†Ø§",
        r"Ù‡ÛŒÚ†.*Ø§Ù…ÛŒØ¯ÛŒ.*Ù†ÛŒØ³Øª",
        r"Ø®ÙˆØ¯Ú©Ø´ÛŒ.*Ù…ÛŒâ€ŒÚ©Ù†Ù…"
    ],
    "medical_advice": [
        r"Ø¯Ø§Ø±ÙˆÛŒ.*Ø¨Ø®ÙˆØ±",
        r"Ù†Ø³Ø®Ù‡.*Ø¨Ø¯Ù‡",
        r"ØªØ´Ø®ÛŒØµ.*Ø¨ÛŒÙ…Ø§Ø±ÛŒ",
        r"Ø¯Ø±Ù…Ø§Ù†.*Ù¾Ø²Ø´Ú©ÛŒ"
    ],
    "personal_information": [
        r"Ø¢Ø¯Ø±Ø³.*Ù…Ù†",
        r"Ø´Ù…Ø§Ø±Ù‡.*ØªÙ„ÙÙ†",
        r"Ú©Ø¯.*Ù…Ù„ÛŒ",
        r"Ø´Ù…Ø§Ø±Ù‡.*Ú©Ø§Ø±Øª"
    ],
    "harmful_content": [
        r"Ø®Ø´ÙˆÙ†Øª.*ÙÛŒØ²ÛŒÚ©ÛŒ",
        r"Ø¢Ø²Ø§Ø±.*Ø¬Ù†Ø³ÛŒ",
        r"Ù…ÙˆØ§Ø¯.*Ù…Ø®Ø¯Ø±",
        r"Ø³Ù„Ø§Ø­.*Ú¯Ø±Ù…"
    ]
}

def evaluate_ethics(text: str) -> Dict[str, Any]:
    """Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø³Ø§Ø¯Ù‡ Ùˆ Ù…Ø¤Ø«Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ø®Ù„Ø§Ù‚ÛŒ"""
    
    for rule in RULES:
        if re.search(rule["pattern"], text, re.IGNORECASE):
            result = {
                "timestamp": datetime.now().isoformat(),
                "rule_id": rule["id"],
                "action": rule["action"],
                "level": rule["level"],
                "text": text[:100],
                "message": rule["message"]
            }
            
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡
            if MEMORY_AVAILABLE:
                try:
                    add_memory("ethics_evaluation", "Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø³ÛŒØ³ØªÙ…", result)
                except Exception as e:
                    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§ÙØ¸Ù‡: {e}")
            
            return result
    
    return {"action": "approve", "level": "safe", "message": ""}

def ethical_guard(answer: str) -> str:
    """Ù…Ø­Ø§ÙØ¸ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§"""
    ethics = evaluate_ethics(answer)
    
    if ethics["action"] == "alert":
        return "âš ï¸ Ø§ÛŒÙ† Ù…ÙˆØ¶ÙˆØ¹ Ø­Ø³Ø§Ø³ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø¨Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ ÙˆØ§Ù‚Ø¹ÛŒ ØµØ­Ø¨Øª Ú©Ù†ÛŒØ¯."
    elif ethics["action"] == "deny":
        return "ğŸš« Ù…Ù† Ù†Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¯Ø± Ù…ÙˆØ±Ø¯ Ù…Ø³Ø§Ø¦Ù„ Ø¯Ø§Ø±ÙˆÛŒÛŒ ÛŒØ§ Ù¾Ø²Ø´Ú©ÛŒ Ø§Ø¸Ù‡Ø§Ø± Ù†Ø¸Ø± Ú©Ù†Ù…."
    elif ethics["action"] == "redirect":
        return "ğŸ’¬ Ø¨Ù‡ØªØ± Ø§Ø³Øª Ø¯Ø±Ø¨Ø§Ø±Ù‡â€ŒÛŒ Ø§Ø­Ø³Ø§Ø³Ø§Øª Ùˆ ØªØ¬Ø±Ø¨Ù‡â€ŒÙ‡Ø§ÛŒØª ØµØ­Ø¨Øª Ú©Ù†ÛŒÙ…ØŒ Ù†Ù‡ Ù…ÙˆØ¶ÙˆØ¹Ø§Øª Ø®Ø§Ø±Ø¬ Ø§Ø² Ø±ÙˆØ§Ù†â€ŒØ´Ù†Ø§Ø³ÛŒ."
    else:
        return answer

def analyze_tone(content: str) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ Ù„Ø­Ù† Ùˆ ØªÙ† Ù…Ø­ØªÙˆØ§"""
    
    # Ú©Ù„Ù…Ø§Øª Ù…Ø«Ø¨Øª
    positive_words = [
        "Ø®ÙˆØ¨", "Ø¹Ø§Ù„ÛŒ", "Ù…Ø«Ø¨Øª", "Ø§Ù…ÛŒØ¯ÙˆØ§Ø±", "Ø®ÙˆØ´Ø­Ø§Ù„", "Ø±Ø§Ø¶ÛŒ", "Ù…Ø·Ù…Ø¦Ù†",
        "Ù…ÙˆÙÙ‚", "Ù¾ÛŒØ´Ø±ÙØª", "Ø¨Ù‡Ø¨ÙˆØ¯", "Ú©Ù…Ú©", "Ø­Ù…Ø§ÛŒØª", "Ù‡Ù…Ø¯Ù„ÛŒ"
    ]
    
    # Ú©Ù„Ù…Ø§Øª Ù…Ù†ÙÛŒ
    negative_words = [
        "Ø¨Ø¯", "Ù…Ù†ÙÛŒ", "Ù†Ø§Ø§Ù…ÛŒØ¯", "ØºÙ…Ú¯ÛŒÙ†", "Ø¹ØµØ¨Ø§Ù†ÛŒ", "Ù†Ø§Ø±Ø§Ø¶ÛŒ", "Ù…Ø¶Ø·Ø±Ø¨",
        "Ù†Ø§Ù…ÙˆÙÙ‚", "Ø´Ú©Ø³Øª", "Ø¨Ø¯ØªØ±", "Ù…Ø´Ú©Ù„", "Ø¯Ø±Ø¯", "Ø±Ù†Ø¬"
    ]
    
    content_lower = content.lower()
    
    positive_count = sum(1 for word in positive_words if word in content_lower)
    negative_count = sum(1 for word in negative_words if word in content_lower)
    
    total_words = len(content.split())
    positive_score = positive_count / max(total_words, 1)
    negative_score = negative_count / max(total_words, 1)
    
    # ØªØ¹ÛŒÛŒÙ† Ù„Ø­Ù† ØºØ§Ù„Ø¨
    if positive_score > negative_score:
        dominant_tone = "positive"
    elif negative_score > positive_score:
        dominant_tone = "negative"
    else:
        dominant_tone = "neutral"
    
    return {
        "positive_score": round(positive_score, 3),
        "negative_score": round(negative_score, 3),
        "dominant_tone": dominant_tone,
        "positive_words_count": positive_count,
        "negative_words_count": negative_count
    }

def get_ethics_statistics() -> Dict[str, Any]:
    """Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø± Ø§Ø®Ù„Ø§Ù‚ÛŒ"""
    
    if not MEMORY_AVAILABLE:
        return {"status": "no_memory", "message": "Ø­Ø§ÙØ¸Ù‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"}
    
    try:
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø®Ù„Ø§Ù‚ÛŒ
        ethics_checks = retrieve_memory("Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø®Ù„Ø§Ù‚ÛŒ", top_k=100, memory_type="ethics_check")
        
        if not ethics_checks:
            return {"status": "no_data", "message": "Ù‡ÛŒÚ† Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø®Ù„Ø§Ù‚ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯"}
        
        # ØªØ­Ù„ÛŒÙ„ Ø¢Ù…Ø§Ø±
        total_checks = len(ethics_checks)
        approved_count = 0
        blocked_count = 0
        flagged_count = 0
        warning_count = 0
        
        ethics_scores = []
        violation_types = defaultdict(int)
        
        for check in ethics_checks:
            memory = check["memory"]
            metadata = memory.get("metadata", {})
            
            status = metadata.get("status", "unknown")
            ethics_score = metadata.get("ethics_score", 0)
            violations_count = metadata.get("violations_count", 0)
            
            if status == "approved":
                approved_count += 1
            elif status == "blocked":
                blocked_count += 1
            elif status == "flagged":
                flagged_count += 1
            elif status == "warning":
                warning_count += 1
            
            if ethics_score > 0:
                ethics_scores.append(ethics_score)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¢Ù…Ø§Ø±
        avg_ethics_score = sum(ethics_scores) / len(ethics_scores) if ethics_scores else 0
        approval_rate = approved_count / total_checks if total_checks > 0 else 0
        
        stats = {
            "total_checks": total_checks,
            "approved_count": approved_count,
            "blocked_count": blocked_count,
            "flagged_count": flagged_count,
            "warning_count": warning_count,
            "approval_rate": round(approval_rate, 3),
            "avg_ethics_score": round(avg_ethics_score, 3),
            "violation_types": dict(violation_types),
            "ethics_level": "high" if avg_ethics_score > 0.8 else "medium" if avg_ethics_score > 0.6 else "low"
        }
        
        return {"status": "success", "stats": stats}
        
    except Exception as e:
        return {"status": "error", "message": f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¢Ù…Ø§Ø±: {e}"}

def suggest_ethical_improvement(content: str) -> List[str]:
    """Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ù‡Ø¨ÙˆØ¯ Ø§Ø®Ù„Ø§Ù‚ÛŒ Ù…Ø­ØªÙˆØ§"""
    
    suggestions = []
    content_lower = content.lower()
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù„Ø­Ù† Ù…Ù†ÙÛŒ
    if any(word in content_lower for word in ["Ø¨Ø¯", "Ù…Ù†ÙÛŒ", "Ù†Ø§Ø§Ù…ÛŒØ¯"]):
        suggestions.append("Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ù…Ø«Ø¨Øªâ€ŒØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø·ÙˆÙ„ Ù¾Ø§Ø³Ø®
    if len(content) > 800:
        suggestions.append("Ù¾Ø§Ø³Ø® Ø±Ø§ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ú©Ù†ÛŒØ¯ ØªØ§ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù…â€ŒØªØ± Ø¨Ø§Ø´Ø¯")
    
    # Ø¨Ø±Ø±Ø³ÛŒ ØªØ®ØµØµÛŒ Ø¨ÙˆØ¯Ù†
    if any(word in content_lower for word in ["ØªØ´Ø®ÛŒØµ", "Ø¯Ø§Ø±Ùˆ", "Ø¯Ø±Ù…Ø§Ù†"]):
        suggestions.append("Ø§Ø² Ø§Ø±Ø§Ø¦Ù‡ Ù…Ø´Ø§ÙˆØ±Ù‡ Ù¾Ø²Ø´Ú©ÛŒ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø±ÛŒÙ… Ø®ØµÙˆØµÛŒ
    if any(word in content_lower for word in ["Ø¢Ø¯Ø±Ø³", "ØªÙ„ÙÙ†", "Ú©Ø¯ Ù…Ù„ÛŒ"]):
        suggestions.append("Ø§Ø² Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø®ØµÛŒ Ø®ÙˆØ¯Ø¯Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯")
    
    return suggestions

def audit_ethics_system() -> Dict[str, Any]:
    """Ø­Ø³Ø§Ø¨Ø±Ø³ÛŒ Ø³ÛŒØ³ØªÙ… Ø§Ø®Ù„Ø§Ù‚ÛŒ"""
    
    audit_result = {
        "timestamp": datetime.now().isoformat(),
        "rules_count": len(ETHICS_RULES),
        "patterns_count": len(ETHICS_PATTERNS),
        "system_status": "active",
        "recommendations": []
    }
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§Ø®Ù„Ø§Ù‚ÛŒ
    for rule_name, rule_data in ETHICS_RULES.items():
        if not rule_data.get("keywords"):
            audit_result["recommendations"].append(f"Ù‚Ø§Ù†ÙˆÙ† {rule_name} ÙØ§Ù‚Ø¯ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø³Øª")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù„Ú¯ÙˆÙ‡Ø§ÛŒ ØªØ´Ø®ÛŒØµÛŒ
    for pattern_name, patterns in ETHICS_PATTERNS.items():
        if not patterns:
            audit_result["recommendations"].append(f"Ø§Ù„Ú¯ÙˆÛŒ {pattern_name} ÙØ§Ù‚Ø¯ Ø§Ù„Ú¯Ùˆ Ø§Ø³Øª")
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø­Ø§ÙØ¸Ù‡
    if MEMORY_AVAILABLE:
        try:
            memory_stats = get_memory_stats()
            audit_result["memory_status"] = "connected"
            audit_result["memory_stats"] = memory_stats
        except Exception as e:
            audit_result["memory_status"] = "error"
            audit_result["memory_error"] = str(e)
    else:
        audit_result["memory_status"] = "disconnected"
        audit_result["recommendations"].append("Ø­Ø§ÙØ¸Ù‡ Ø§Ø®Ù„Ø§Ù‚ÛŒ ØºÛŒØ±ÙØ¹Ø§Ù„ Ø§Ø³Øª")
    
    return audit_result

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("âš–ï¸ Ø³ÛŒØ³ØªÙ… AI Ethics & Self-Regulation - Testology")
        print("ğŸ§  ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ÙˆØ¬ÙˆØ¯ Ø®ÙˆØ¯Ù†Ø§Ø¸Ø±")
        print("=" * 50)
        
        # ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ø§Ø®Ù„Ø§Ù‚ÛŒ
        test_cases = [
            "Ù…Ù† Ø¯Ø§Ø±Ù… Ø¨Ù‡ Ù…Ø±Ú¯ ÙÚ©Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…",
            "Ú†Ù‡ Ø¯Ø§Ø±ÙˆÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯ØŸ",
            "Ø¢Ø¯Ø±Ø³ Ù…Ù†Ø²Ù„ Ù…Ù† Ú©Ø¬Ø§Ø³ØªØŸ",
            "Ø§Ù…Ø±ÙˆØ² Ø±ÙˆØ² Ø®ÙˆØ¨ÛŒ Ø¨ÙˆØ¯ Ùˆ Ø§Ø­Ø³Ø§Ø³ Ù…Ø«Ø¨ØªÛŒ Ø¯Ø§Ø±Ù…",
            "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù… Ø¯Ø± Ø²Ù†Ø¯Ú¯ÛŒ Ù¾ÛŒØ´Ø±ÙØª Ú©Ù†Ù…"
        ]
        
        print("ğŸ§ª ØªØ³Øª Ø³ÛŒØ³ØªÙ… Ø§Ø®Ù„Ø§Ù‚ÛŒ:")
        for text in test_cases:
            result = evaluate_ethics(text)
            print(f"\nğŸ“ ØªØ³Øª: {text[:50]}...")
            print(f"   Ø¹Ù…Ù„: {result['action']}")
            print(f"   Ø³Ø·Ø­: {result['level']}")
            if result.get('message'):
                print(f"   Ù¾ÛŒØ§Ù…: {result['message']}")
        
        print(f"\nğŸ›¡ï¸ ØªØ³Øª Ù…Ø­Ø§ÙØ¸ Ø§Ø®Ù„Ø§Ù‚ÛŒ:")
        for text in test_cases:
            guarded = ethical_guard(text)
            print(f"\nğŸ“ ÙˆØ±ÙˆØ¯ÛŒ: {text[:50]}...")
            print(f"ğŸ›¡ï¸ Ø®Ø±ÙˆØ¬ÛŒ: {guarded[:100]}...")
        
        print(f"\nâš–ï¸ Testology Ø­Ø§Ù„Ø§ ÛŒÚ© Ù…ÙˆØ¬ÙˆØ¯ Ø®ÙˆØ¯Ù†Ø§Ø¸Ø± Ø§Ø³Øª!")
    
    else:
        # API Mode
        command = sys.argv[1]
        
        if command == "evaluate_ethics":
            text = sys.argv[2]
            result = evaluate_ethics(text)
            print(json.dumps(result, ensure_ascii=False))
            
        elif command == "ethical_guard":
            answer = sys.argv[2]
            guarded = ethical_guard(answer)
            print(json.dumps({"guarded_answer": guarded}, ensure_ascii=False))
