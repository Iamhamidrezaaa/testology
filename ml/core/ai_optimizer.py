#!/usr/bin/env python3
"""
Ø³ÛŒØ³ØªÙ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø¯Ù„ Testology
Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
"""

import json
import os
import sys
import joblib
import pandas as pd
import numpy as np
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import LabelEncoder, MinMaxScaler

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ø³ÛŒØ± utils
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

LOG_PATH = "ml/data/optimization_log.json"
MODEL_PATH = "ml/core/model.pkl"
BACKUP_PATH = "ml/core/model_backup_optimized.pkl"

def load_data_and_preprocess():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
    try:
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        df = pd.read_csv("ml/data/user_tests.csv")
        df = df.dropna()
        df = df[df['score'].between(0, 100)]
        df = df[df['age'].between(18, 100)]
        
        # Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ø¬Ù†Ø³ÛŒØª
        le = LabelEncoder()
        df["gender_encoded"] = le.fit_transform(df["gender"].astype(str))
        
        # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù…Ø±Ù‡
        scaler = MinMaxScaler()
        df["score_scaled"] = scaler.fit_transform(df[["score"]])
        
        # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
        X = df[["score_scaled", "gender_encoded", "age"]]
        y = df["category"]
        
        return X, y, le, scaler
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {e}")
        return None, None, None, None

def create_parameter_grid():
    """Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¨Ú©Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
    return {
        "n_estimators": [100, 150, 200, 250, 300],
        "max_depth": [6, 8, 10, 12, 15, None],
        "min_samples_split": [2, 4, 6, 8, 10],
        "min_samples_leaf": [1, 2, 4, 6],
        "max_features": ["sqrt", "log2", None],
        "bootstrap": [True, False],
        "class_weight": ["balanced", "balanced_subsample", None]
    }

def optimize_with_grid_search(X, y):
    """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ GridSearchCV"""
    print("ğŸ” Ø´Ø±ÙˆØ¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ GridSearchCV...")
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y
    )
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡
    model = RandomForestClassifier(random_state=42, n_jobs=-1)
    
    # Ø´Ø¨Ú©Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
    param_grid = create_parameter_grid()
    
    # GridSearchCV
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        cv=3,
        n_jobs=-1,
        verbose=1,
        scoring='accuracy'
    )
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
    grid_search.fit(X_train, y_train)
    
    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡
    best_model = grid_search.best_estimator_
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    return best_model, grid_search.best_params_, accuracy, grid_search.best_score_

def optimize_with_random_search(X, y):
    """Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ RandomizedSearchCV"""
    print("ğŸ” Ø´Ø±ÙˆØ¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ RandomizedSearchCV...")
    
    # ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42, stratify=y
    )
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡
    model = RandomForestClassifier(random_state=42, n_jobs=-1)
    
    # Ø´Ø¨Ú©Ù‡ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§
    param_dist = create_parameter_grid()
    
    # RandomizedSearchCV
    random_search = RandomizedSearchCV(
        estimator=model,
        param_distributions=param_dist,
        n_iter=50,
        cv=3,
        n_jobs=-1,
        verbose=1,
        scoring='accuracy',
        random_state=42
    )
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ
    random_search.fit(X_train, y_train)
    
    # Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡
    best_model = random_search.best_estimator_
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    
    return best_model, random_search.best_params_, accuracy, random_search.best_score_

def backup_existing_model():
    """Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø¯Ù„ Ù…ÙˆØ¬ÙˆØ¯"""
    if os.path.exists(MODEL_PATH):
        import shutil
        shutil.copy2(MODEL_PATH, BACKUP_PATH)
        return True
    return False

def save_optimization_log(best_params, accuracy, cv_score, method):
    """Ø°Ø®ÛŒØ±Ù‡ Ù„Ø§Ú¯ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"""
    log_entry = {
        "timestamp": datetime.now().isoformat(),
        "method": method,
        "best_params": best_params,
        "accuracy": round(accuracy, 4),
        "cv_score": round(cv_score, 4),
        "improvement": "N/A"  # Ø¯Ø± Ø§ÙˆÙ„ÛŒÙ† Ø§Ø¬Ø±Ø§
    }
    
    # Ø®ÙˆØ§Ù†Ø¯Ù† ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…ÙˆØ¬ÙˆØ¯
    if os.path.exists(LOG_PATH):
        with open(LOG_PATH, 'r', encoding='utf-8') as f:
            history = json.load(f)
    else:
        history = []
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ø§Ø¬Ø±Ø§ÛŒ Ù‚Ø¨Ù„ÛŒ
    if history:
        last_accuracy = history[-1].get('accuracy', 0)
        improvement = accuracy - last_accuracy
        log_entry["improvement"] = round(improvement, 4)
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÙˆØ±ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯
    history.append(log_entry)
    
    # Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ† ÙÙ‚Ø· 50 ÙˆØ±ÙˆØ¯ÛŒ Ø¢Ø®Ø±
    if len(history) > 50:
        history = history[-50:]
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù„Ø§Ú¯
    os.makedirs("ml/data", exist_ok=True)
    with open(LOG_PATH, 'w', encoding='utf-8') as f:
        json.dump(history, f, ensure_ascii=False, indent=2)

def optimize_model():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø¯Ù„"""
    try:
        print("âš™ï¸ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø¯Ù„ Testology")
        print("=" * 50)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        X, y, le, scaler = load_data_and_preprocess()
        if X is None:
            return {"status": "error", "message": "Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"}
        
        print(f"ğŸ“Š Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ {len(X)} Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡")
        
        # Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ø¯Ù„ Ù…ÙˆØ¬ÙˆØ¯
        backup_created = backup_existing_model()
        
        # Ø§Ù†ØªØ®Ø§Ø¨ Ø±ÙˆØ´ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡
        if len(X) < 1000:
            print("ğŸ“ˆ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡ Ú©Ù… - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² GridSearchCV")
            best_model, best_params, accuracy, cv_score = optimize_with_grid_search(X, y)
            method = "GridSearchCV"
        else:
            print("ğŸ“ˆ Ø­Ø¬Ù… Ø¯Ø§Ø¯Ù‡ Ø²ÛŒØ§Ø¯ - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² RandomizedSearchCV")
            best_model, best_params, accuracy, cv_score = optimize_with_random_search(X, y)
            method = "RandomizedSearchCV"
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø¨Ù‡ÛŒÙ†Ù‡
        os.makedirs("ml/core", exist_ok=True)
        joblib.dump(best_model, MODEL_PATH)
        joblib.dump(le, "ml/core/label_encoder.pkl")
        joblib.dump(scaler, "ml/core/scaler.pkl")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù„Ø§Ú¯
        save_optimization_log(best_params, accuracy, cv_score, method)
        
        # Ú¯Ø²Ø§Ø±Ø´ ØªÙØµÛŒÙ„ÛŒ
        report = classification_report(y, best_model.predict(X), output_dict=True)
        
        result = {
            "status": "success",
            "message": "Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯",
            "method": method,
            "best_params": best_params,
            "accuracy": round(accuracy, 4),
            "cv_score": round(cv_score, 4),
            "backup_created": backup_created,
            "timestamp": datetime.now().isoformat(),
            "model_info": {
                "n_estimators": best_params.get('n_estimators'),
                "max_depth": best_params.get('max_depth'),
                "min_samples_split": best_params.get('min_samples_split'),
                "min_samples_leaf": best_params.get('min_samples_leaf'),
                "max_features": best_params.get('max_features'),
                "bootstrap": best_params.get('bootstrap'),
                "class_weight": best_params.get('class_weight')
            },
            "classification_report": report
        }
        
        print("âœ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
        print(f"ğŸ“ˆ Ø¯Ù‚Øª: {accuracy:.4f}")
        print(f"ğŸ¯ Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡: {best_params}")
        print(f"ğŸ’¾ Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯: {backup_created}")
        
        return result
        
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: {e}")
        return error_result

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    print("ğŸ§  Ø³ÛŒØ³ØªÙ… Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Testology")
    print("=" * 40)
    
    result = optimize_model()
    
    # Ø®Ø±ÙˆØ¬ÛŒ JSON Ø¨Ø±Ø§ÛŒ API
    print(json.dumps(result, ensure_ascii=False))

if __name__ == "__main__":
    main()