import json
import os
from datetime import datetime
from sentence_transformers import SentenceTransformer, util
import torch
import numpy as np

# Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§ÙØ¸Ù‡
MEMORY_PATH = "ml/data/neural_memory.json"

# Ù…Ø¯Ù„ Sentence Transformer Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Embedding
model = SentenceTransformer("all-MiniLM-L6-v2")

def load_memory():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø­Ø§ÙØ¸Ù‡ Ø§Ø² ÙØ§ÛŒÙ„ JSON"""
    if os.path.exists(MEMORY_PATH):
        with open(MEMORY_PATH, 'r', encoding='utf-8') as f:
            return json.load(f)
    return []

def save_memory(memories):
    """Ø°Ø®ÛŒØ±Ù‡ Ø­Ø§ÙØ¸Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ JSON"""
    os.makedirs(os.path.dirname(MEMORY_PATH), exist_ok=True)
    with open(MEMORY_PATH, 'w', encoding='utf-8') as f:
        json.dump(memories, f, ensure_ascii=False, indent=2)

def add_memory(event_type, content, metadata=None):
    """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ØªØ¬Ø±Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ø­Ø§ÙØ¸Ù‡"""
    # Ø§ÛŒØ¬Ø§Ø¯ Embedding Ø¨Ø±Ø§ÛŒ Ù…Ø­ØªÙˆØ§
    embeddings = model.encode(content, convert_to_tensor=True).tolist()
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø­Ø§ÙØ¸Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
    memories = load_memory()
    
    # Ø§ÛŒØ¬Ø§Ø¯ ÙˆØ±ÙˆØ¯ÛŒ Ø¬Ø¯ÛŒØ¯
    entry = {
        "id": len(memories) + 1,
        "timestamp": datetime.now().isoformat(),
        "type": event_type,
        "content": content,
        "metadata": metadata or {},
        "embedding": embeddings,
        "importance": calculate_importance(content, event_type),
        "emotion": extract_emotion(content)
    }
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø­Ø§ÙØ¸Ù‡
    memories.append(entry)
    save_memory(memories)
    
    print(f"ğŸ§  Ø­Ø§ÙØ¸Ù‡ Ø¬Ø¯ÛŒØ¯ Ø«Ø¨Øª Ø´Ø¯: {event_type} - {content[:50]}...")
    return entry

def retrieve_memory(query, top_k=3, memory_type=None):
    """Ø¨Ø§Ø²ÛŒØ§Ø¨ÛŒ Ø­Ø§ÙØ¸Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø¨Ø§Ù‡Øª Ù…Ø¹Ù†Ø§ÛŒÛŒ"""
    memories = load_memory()
    if not memories:
        return []
    
    # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ø­Ø§ÙØ¸Ù‡ Ø§Ú¯Ø± Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
    if memory_type:
        memories = [m for m in memories if m.get("type") == memory_type]
    
    if not memories:
        return []
    
    # Ø§ÛŒØ¬Ø§Ø¯ Embedding Ø¨Ø±Ø§ÛŒ Ú©ÙˆØ¦Ø±ÛŒ
    query_emb = model.encode(query, convert_to_tensor=True)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ú©Ø³ÛŒÙ†ÙˆØ³
    similarities = []
    for memory in memories:
        memory_emb = torch.tensor(memory["embedding"])
        similarity = util.pytorch_cos_sim(query_emb, memory_emb).item()
        
        # ØªØ±Ú©ÛŒØ¨ Ø´Ø¨Ø§Ù‡Øª Ø¨Ø§ Ø§Ù‡Ù…ÛŒØª Ø­Ø§ÙØ¸Ù‡
        weighted_score = similarity * (1 + memory.get("importance", 0.5))
        
        similarities.append({
            "memory": memory,
            "similarity": round(similarity, 3),
            "weighted_score": round(weighted_score, 3)
        })
    
    # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² ÙˆØ²Ù†ÛŒ
    similarities.sort(key=lambda x: x["weighted_score"], reverse=True)
    
    return similarities[:top_k]

def calculate_importance(content, event_type):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù‡Ù…ÛŒØª Ø­Ø§ÙØ¸Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­ØªÙˆØ§ Ùˆ Ù†ÙˆØ¹ Ø±ÙˆÛŒØ¯Ø§Ø¯"""
    importance_weights = {
        "decision": 0.9,
        "emotion": 0.8,
        "test_result": 0.7,
        "chat": 0.6,
        "feedback": 0.8,
        "supervisor_analysis": 0.9
    }
    
    base_importance = importance_weights.get(event_type, 0.5)
    
    # Ø§ÙØ²Ø§ÛŒØ´ Ø§Ù‡Ù…ÛŒØª Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ
    important_keywords = ["Ù…Ù‡Ù…", "Ø­ÛŒØ§ØªÛŒ", "ØªØºÛŒÛŒØ±", "ØªØµÙ…ÛŒÙ…", "Ø§Ø­Ø³Ø§Ø³", "Ù†ØªÛŒØ¬Ù‡"]
    keyword_boost = sum(1 for keyword in important_keywords if keyword in content) * 0.1
    
    return min(base_importance + keyword_boost, 1.0)

def extract_emotion(content):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø­Ø³Ø§Ø³ Ø§Ø² Ù…Ø­ØªÙˆØ§"""
    emotion_keywords = {
        "Ø®ÙˆØ´Ø­Ø§Ù„ÛŒ": ["Ø®ÙˆØ´Ø­Ø§Ù„", "Ø´Ø§Ø¯", "Ø±Ø§Ø¶ÛŒ", "Ù…Ø«Ø¨Øª"],
        "ØºÙ…": ["ØºÙ…Ú¯ÛŒÙ†", "Ù†Ø§Ø±Ø§Ø­Øª", "ØºÙ…", "Ø§Ù†Ø¯ÙˆÙ‡"],
        "Ø§Ø¶Ø·Ø±Ø§Ø¨": ["Ø§Ø¶Ø·Ø±Ø§Ø¨", "Ù†Ú¯Ø±Ø§Ù†", "Ø§Ø³ØªØ±Ø³", "ØªØ±Ø³"],
        "Ø¹ØµØ¨Ø§Ù†ÛŒØª": ["Ø¹ØµØ¨Ø§Ù†ÛŒ", "Ø®Ø´Ù…", "Ù†Ø§Ø±Ø§Ø¶ÛŒ", "Ø¹ØµØ¨Ø§Ù†ÛŒ"],
        "Ø¢Ø±Ø§Ù…Ø´": ["Ø¢Ø±Ø§Ù…", "Ø±Ø§Ø­Øª", "Ø³Ú©ÙˆÙ†", "Ø¢Ø³ÙˆØ¯Ù‡"]
    }
    
    content_lower = content.lower()
    emotions = []
    
    for emotion, keywords in emotion_keywords.items():
        if any(keyword in content_lower for keyword in keywords):
            emotions.append(emotion)
    
    return emotions if emotions else ["Ø®Ù†Ø«ÛŒ"]

def get_memory_stats():
    """Ø¢Ù…Ø§Ø± Ø­Ø§ÙØ¸Ù‡"""
    memories = load_memory()
    if not memories:
        return {"total": 0}
    
    stats = {
        "total": len(memories),
        "by_type": {},
        "by_emotion": {},
        "recent": len([m for m in memories if 
                      (datetime.now() - datetime.fromisoformat(m["timestamp"])).days <= 7])
    }
    
    for memory in memories:
        # Ø¢Ù…Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
        memory_type = memory.get("type", "unknown")
        stats["by_type"][memory_type] = stats["by_type"].get(memory_type, 0) + 1
        
        # Ø¢Ù…Ø§Ø± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø­Ø³Ø§Ø³
        emotions = memory.get("emotion", ["Ø®Ù†Ø«ÛŒ"])
        for emotion in emotions:
            stats["by_emotion"][emotion] = stats["by_emotion"].get(emotion, 0) + 1
    
    return stats

def clear_old_memories(days=30):
    """Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø­Ø§ÙØ¸Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ"""
    memories = load_memory()
    cutoff_date = datetime.now().timestamp() - (days * 24 * 60 * 60)
    
    filtered_memories = []
    removed_count = 0
    
    for memory in memories:
        memory_date = datetime.fromisoformat(memory["timestamp"]).timestamp()
        if memory_date > cutoff_date or memory.get("importance", 0) > 0.8:
            filtered_memories.append(memory)
        else:
            removed_count += 1
    
    save_memory(filtered_memories)
    print(f"ğŸ§¹ {removed_count} Ø­Ø§ÙØ¸Ù‡ Ù‚Ø¯ÛŒÙ…ÛŒ Ù¾Ø§Ú© Ø´Ø¯")
    return removed_count

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ğŸ§  ØªØ³Øª Ø³ÛŒØ³ØªÙ… Neural Memory...")
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø­Ø§ÙØ¸Ù‡â€ŒÙ‡Ø§ÛŒ Ù†Ù…ÙˆÙ†Ù‡
        add_memory("chat", "Ú©Ø§Ø±Ø¨Ø± Ø§Ø­Ø³Ø§Ø³ ØºÙ… Ùˆ Ø§Ø¶Ø·Ø±Ø§Ø¨ Ø¯Ø§Ø±Ø¯ Ùˆ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø­Ù…Ø§ÛŒØª Ø¯Ø§Ø±Ø¯", 
                   {"user_id": 1, "session_id": "sess_001"})
        
        add_memory("test_result", "Ù†ØªÛŒØ¬Ù‡ ØªØ³Øª Ø§Ø¶Ø·Ø±Ø§Ø¨: 75/100 - Ø³Ø·Ø­ Ø¨Ø§Ù„Ø§", 
                   {"test_type": "anxiety", "score": 75})
        
        add_memory("decision", "MetaLearner ØªØµÙ…ÛŒÙ… Ú¯Ø±ÙØª ØªÙ…Ø±ÛŒÙ†Ø§Øª ØªÙ†ÙØ³ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¯Ù‡Ø¯", 
                   {"action": "breathing_exercises", "confidence": 0.85})
        
        # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø­Ø§ÙØ¸Ù‡
        print("\nğŸ” Ø¬Ø³ØªØ¬ÙˆÛŒ Ø­Ø§ÙØ¸Ù‡ Ø¨Ø±Ø§ÛŒ 'Ø§Ø­Ø³Ø§Ø³ ØºÙ… Ùˆ Ø§Ø¶Ø·Ø±Ø§Ø¨':")
        results = retrieve_memory("Ø§Ø­Ø³Ø§Ø³ ØºÙ… Ùˆ Ø§Ø¶Ø·Ø±Ø§Ø¨")
        for result in results:
            print(f"- {result['memory']['content']} (Ø§Ù…ØªÛŒØ§Ø²: {result['weighted_score']})")
        
        # Ø¢Ù…Ø§Ø± Ø­Ø§ÙØ¸Ù‡
        print("\nğŸ“Š Ø¢Ù…Ø§Ø± Ø­Ø§ÙØ¸Ù‡:")
        stats = get_memory_stats()
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„: {stats['total']}")
        print(f"Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹: {stats['by_type']}")
        print(f"Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø­Ø³Ø§Ø³: {stats['by_emotion']}")
    
    else:
        # API Mode
        command = sys.argv[1]
        
        if command == "add_memory":
            event_type = sys.argv[2]
            content = sys.argv[3]
            metadata = json.loads(sys.argv[4]) if len(sys.argv) > 4 else {}
            
            result = add_memory(event_type, content, metadata)
            print(json.dumps(result, ensure_ascii=False))
            
        elif command == "retrieve_memory":
            query = sys.argv[2]
            top_k = int(sys.argv[3]) if len(sys.argv) > 3 else 3
            memory_type = sys.argv[4] if len(sys.argv) > 4 else None
            
            results = retrieve_memory(query, top_k, memory_type)
            print(json.dumps(results, ensure_ascii=False))
            
        elif command == "get_memory_stats":
            stats = get_memory_stats()
            print(json.dumps(stats, ensure_ascii=False))
            
        elif command == "clear_old_memories":
            days = int(sys.argv[2]) if len(sys.argv) > 2 else 30
            removed_count = clear_old_memories(days)
            print(json.dumps({"removed_count": removed_count}, ensure_ascii=False))
