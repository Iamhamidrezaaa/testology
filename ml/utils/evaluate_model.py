#!/usr/bin/env python3
"""
Ø³ÛŒØ³ØªÙ… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Testology
Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù…Ø¯Ù„ Ø±Ø§ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ú¯Ø²Ø§Ø±Ø´ ØªÙØµÛŒÙ„ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
"""

import joblib
import pandas as pd
import numpy as np
import json
import os
from datetime import datetime
from sklearn.metrics import (
    accuracy_score, 
    confusion_matrix, 
    classification_report,
    precision_recall_fscore_support,
    roc_auc_score
)
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score

def load_model_and_data():
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"""
    try:
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
        model = joblib.load("ml/core/model.pkl")
        le = joblib.load("ml/core/label_encoder.pkl")
        scaler = joblib.load("ml/core/scaler.pkl")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        df = pd.read_csv("ml/data/user_tests.csv")
        
        return model, le, scaler, df
    except FileNotFoundError as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ: {e}")
        return None, None, None, None

def preprocess_for_evaluation(df, le, scaler):
    """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ"""
    # ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    df = df.dropna()
    df = df[df['score'].between(0, 100)]
    df = df[df['age'].between(18, 100)]
    
    # Ú©Ø¯Ú¯Ø°Ø§Ø±ÛŒ Ø¬Ù†Ø³ÛŒØª
    df["gender_encoded"] = le.transform(df["gender"].astype(str))
    
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù†Ù…Ø±Ù‡
    df["score_scaled"] = scaler.transform(df[["score"]])
    
    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
    X = df[["score_scaled", "gender_encoded", "age"]]
    y = df["category"]
    
    return X, y, df

def calculate_advanced_metrics(y_true, y_pred, y_proba=None):
    """Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡"""
    # Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ‡
    accuracy = accuracy_score(y_true, y_pred)
    
    # Ú¯Ø²Ø§Ø±Ø´ ØªÙØµÛŒÙ„ÛŒ
    report = classification_report(y_true, y_pred, output_dict=True)
    
    # Ù…Ø§ØªØ±ÛŒØ³ Ø³Ø±Ø¯Ø±Ú¯Ù…ÛŒ
    labels = np.unique(y_true)
    conf_matrix = confusion_matrix(y_true, y_pred, labels=labels)
    
    # F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
    precision, recall, f1, support = precision_recall_fscore_support(
        y_true, y_pred, labels=labels, average=None
    )
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC (Ø§Ú¯Ø± Ø§Ø­ØªÙ…Ø§Ù„â€ŒÙ‡Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯)
    auc_scores = {}
    if y_proba is not None:
        try:
            # Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡
            if len(labels) > 2:
                auc_scores['macro'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='macro')
                auc_scores['weighted'] = roc_auc_score(y_true, y_proba, multi_class='ovr', average='weighted')
            else:
                auc_scores['binary'] = roc_auc_score(y_true, y_proba[:, 1])
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC: {e}")
    
    return {
        'accuracy': accuracy,
        'confusion_matrix': conf_matrix.tolist(),
        'labels': labels.tolist(),
        'report': report,
        'precision': precision.tolist(),
        'recall': recall.tolist(),
        'f1_scores': f1.tolist(),
        'support': support.tolist(),
        'auc_scores': auc_scores
    }

def analyze_model_performance(metrics):
    """ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„ Ùˆ Ø§Ø±Ø§Ø¦Ù‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª"""
    suggestions = []
    
    # ØªØ­Ù„ÛŒÙ„ Ø¯Ù‚Øª Ú©Ù„ÛŒ
    accuracy = metrics['accuracy']
    if accuracy < 0.7:
        suggestions.append("Ø¯Ù‚Øª Ù…Ø¯Ù„ Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ")
    elif accuracy < 0.8:
        suggestions.append("Ø¯Ù‚Øª Ù…Ø¯Ù„ Ù…ØªÙˆØ³Ø· Ø§Ø³Øª. Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„")
    else:
        suggestions.append("Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø¹Ø§Ù„ÛŒ Ø§Ø³Øª. Ù…Ø¯Ù„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø®ÙˆØ¨ÛŒ Ø¯Ø§Ø±Ø¯")
    
    # ØªØ­Ù„ÛŒÙ„ F1-score Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
    f1_scores = metrics['f1_scores']
    labels = metrics['labels']
    
    for i, (label, f1) in enumerate(zip(labels, f1_scores)):
        if f1 < 0.6:
            suggestions.append(f"Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø¯Ø± Ø¯Ø³ØªÙ‡ '{label}' Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Ø§ÙØ²ÙˆØ¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡")
        elif f1 < 0.8:
            suggestions.append(f"Ø¯Ù‚Øª Ù…Ø¯Ù„ Ø¯Ø± Ø¯Ø³ØªÙ‡ '{label}' Ù…ØªÙˆØ³Ø· Ø§Ø³Øª. Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯: Ø¨Ø±Ø±Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§ÛŒÙ† Ø¯Ø³ØªÙ‡")
    
    # ØªØ­Ù„ÛŒÙ„ Ù…Ø§ØªØ±ÛŒØ³ Ø³Ø±Ø¯Ø±Ú¯Ù…ÛŒ
    conf_matrix = np.array(metrics['confusion_matrix'])
    total_samples = np.sum(conf_matrix)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ø±ØµØ¯ Ø®Ø·Ø§
    error_rate = 1 - accuracy
    suggestions.append(f"Ù†Ø±Ø® Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {error_rate:.2%}")
    
    # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±
    for i, label in enumerate(labels):
        class_errors = np.sum(conf_matrix[i]) - conf_matrix[i, i]
        class_total = np.sum(conf_matrix[i])
        if class_total > 0:
            class_error_rate = class_errors / class_total
            if class_error_rate > 0.3:
                suggestions.append(f"Ø¯Ø³ØªÙ‡ '{label}' Ø®Ø·Ø§ÛŒ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯ ({class_error_rate:.1%})")
    
    return suggestions

def generate_heatmap_data(conf_matrix, labels):
    """ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Heatmap"""
    heatmap_data = []
    
    for i, true_label in enumerate(labels):
        for j, pred_label in enumerate(labels):
            heatmap_data.append({
                'true_label': true_label,
                'predicted_label': pred_label,
                'value': int(conf_matrix[i, j]),
                'percentage': round((conf_matrix[i, j] / np.sum(conf_matrix[i])) * 100, 1) if np.sum(conf_matrix[i]) > 0 else 0
            })
    
    return heatmap_data

def evaluate_model():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„"""
    try:
        print("ğŸ” Ø´Ø±ÙˆØ¹ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„...")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        model, le, scaler, df = load_model_and_data()
        if model is None:
            return {"status": "error", "message": "Ù…Ø¯Ù„ ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ ÛŒØ§ÙØª Ù†Ø´Ø¯"}
        
        print(f"ğŸ“Š Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ {len(df)} Ù†Ù…ÙˆÙ†Ù‡ Ø¯Ø§Ø¯Ù‡")
        
        # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
        X, y, df_processed = preprocess_for_evaluation(df, le, scaler)
        print(f"âœ… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ú©Ø§Ù…Ù„: {X.shape[0]} Ù†Ù…ÙˆÙ†Ù‡")
        
        # Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ
        y_pred = model.predict(X)
        y_proba = model.predict_proba(X) if hasattr(model, 'predict_proba') else None
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØªØ±ÛŒÚ©â€ŒÙ‡Ø§
        metrics = calculate_advanced_metrics(y, y_pred, y_proba)
        
        # ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯
        suggestions = analyze_model_performance(metrics)
        
        # ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Heatmap
        heatmap_data = generate_heatmap_data(
            np.array(metrics['confusion_matrix']), 
            metrics['labels']
        )
        
        # Cross-validation
        try:
            cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')
            metrics['cross_validation'] = {
                'mean': float(np.mean(cv_scores)),
                'std': float(np.std(cv_scores)),
                'scores': cv_scores.tolist()
            }
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Cross-validation: {e}")
            metrics['cross_validation'] = None
        
        # Ù†ØªÛŒØ¬Ù‡ Ù†Ù‡Ø§ÛŒÛŒ
        evaluation_result = {
            "status": "success",
            "timestamp": datetime.now().isoformat(),
            "metrics": metrics,
            "suggestions": suggestions,
            "heatmap_data": heatmap_data,
            "model_info": {
                "n_features": X.shape[1],
                "n_samples": X.shape[0],
                "n_classes": len(metrics['labels']),
                "model_type": type(model).__name__
            }
        }
        
        # Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´
        os.makedirs("ml/data", exist_ok=True)
        with open("ml/data/eval_report.json", 'w', encoding='utf-8') as f:
            json.dump(evaluation_result, f, ensure_ascii=False, indent=2)
        
        print("âœ… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯")
        print(f"ğŸ“ˆ Ø¯Ù‚Øª: {metrics['accuracy']:.3f}")
        print(f"ğŸ“Š ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§: {len(metrics['labels'])}")
        print(f"ğŸ’¡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ø§Øª: {len(suggestions)} Ù…ÙˆØ±Ø¯")
        
        return evaluation_result
        
    except Exception as e:
        error_result = {
            "status": "error",
            "message": f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„: {str(e)}",
            "timestamp": datetime.now().isoformat()
        }
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ: {e}")
        return error_result

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    print("ğŸ§  Ø³ÛŒØ³ØªÙ… Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Testology")
    print("=" * 40)
    
    result = evaluate_model()
    
    # Ø®Ø±ÙˆØ¬ÛŒ JSON Ø¨Ø±Ø§ÛŒ API
    print(json.dumps(result, ensure_ascii=False))

if __name__ == "__main__":
    main()













